{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Random Forests and ExtraTrees\n",
    "\n",
    "\n",
    "_Authors: Matt Brems (DC), Riley Dallas (AUS)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "---\n",
    "\n",
    "With bagged decision trees, we generate many different trees on pretty similar data. These trees are **strongly correlated** with one another. Because these trees are correlated with one another, they will have high variance. Looking at the variance of the average of two random variables $T_1$ and $T_2$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Var\\left(\\frac{T_1+T_2}{2}\\right) &=& \\frac{1}{4}\\left[Var(T_1) + Var(T_2) + 2Cov(T_1,T_2)\\right]\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "If $T_1$ and $T_2$ are highly correlated, then the variance will about as high as we'd see with individual decision trees. By \"de-correlating\" our trees from one another, we can drastically reduce the variance of our model.\n",
    "\n",
    "That's the difference between bagged decision trees and random forests! We're going to do the same thing as before, but we're going to de-correlate our trees. This will reduce our variance (at the expense of a small increase in bias) and thus should greatly improve the overall performance of the final model.\n",
    "\n",
    "So how do we \"de-correlate\" our trees?\n",
    "\n",
    "Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each split in the learning process, a **random subset of the features**. This process is sometimes called the *random subspace method*.\n",
    "\n",
    "The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be used in many/all of the bagged decision trees, causing them to become correlated. By selecting a random subset of features at each split, we counter this correlation between base trees, strengthening the overall model.\n",
    "\n",
    "For a problem with $p$ features, it is typical to use:\n",
    "\n",
    "- $\\sqrt{p}$ (rounded down) features in each split for a classification problem.\n",
    "- $p/3$ (rounded down) with a minimum node size of 5 as the default for a regression problem.\n",
    "\n",
    "While this is a guideline, Hastie and Tibshirani (authors of Introduction to Statistical Learning and Elements of Statistical Learning) have suggested this as a good rule in the absence of some rationale to do something different.\n",
    "\n",
    "Random forests, a step beyond bagged decision trees, are **very widely used** classifiers and regressors. They are relatively simple to use because they require very few parameters to set and they perform pretty well.\n",
    "- It is quite common for interviewers to ask how a random forest is constructed or how it is superior to a single decision tree.\n",
    "\n",
    "--- \n",
    "\n",
    "## Extremely Randomized Trees (ExtraTrees)\n",
    "Adding one more step of randomization (and thus de-correlation) yields extremely randomized trees, or _ExtraTrees_. These are trained using bagging (sampling of observations) and the random subspace method (sampling of features), but an additional layer of randomness is introduced. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity) for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range.\n",
    "\n",
    "This further reduces the variance, but causes an increase in bias. If you're considering using ExtraTrees, you might consider this to be a hyperparameter you can tune. Build an ExtraTrees model and a Random Forest model, then compare their performance!\n",
    "\n",
    "That's exactly what we'll do below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "---\n",
    "\n",
    "We'll need the following libraries for today's lecture:\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `GridSearchCV`, `train_test_split` and `cross_val_score` from `sklearn`'s `model_selection` module \n",
    "- `RandomForestClassifier` and `ExtraTreesClassifier` from `sklearn`'s `ensemble` module "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 31,
=======
   "execution_count": 37,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
<<<<<<< Updated upstream
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV"
=======
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "---\n",
    "\n",
    "Load `train.csv` and `test.csv` from Kaggle into `DataFrames`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/train.csv')"
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('datasets/train.csv')"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('datasets/test.csv')"
=======
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('datasets/test.csv')"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Drop the two rows with missing `Embarked` values from train\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 4,
=======
     "execution_count": 5,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['Embarked'].notnull()]"
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train[train['Embarked'].notnull()]"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(889, 12)"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 6,
=======
     "execution_count": 7,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: `Fare`\n",
    "---\n",
    "\n",
    "The test set has one row with a missing value for `Fare`. Fill it with the average `Fare` with everyone from the same `Pclass`. **Use the training set to calculate the average!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Pclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 12,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "array([3, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
=======
       "152    3\n",
       "Name: Pclass, dtype: int64"
      ]
     },
     "execution_count": 12,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "test['Pclass'].unique()"
=======
    "test.loc[test['Fare'].isnull(),'Pclass']"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 8,
=======
   "execution_count": 15,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "152    3\n",
       "Name: Pclass, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.loc[test['Fare'].isnull(), 'Pclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.675550101832997"
      ]
     },
     "execution_count": 9,
=======
       "13.675550101832997"
      ]
     },
     "execution_count": 15,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "mean_fare_3 = train[train['Pclass']==3]['Fare'].mean()\n",
    "mean_fare_3"
=======
    "mean_fair_3=train[train['Pclass']==3]['Fare'].mean()\n",
    "mean_fair_3"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Fare'] = test['Fare'].fillna(mean_fare_3)"
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Fair']=test['Fare'].fillna(mean_fair_3)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: `Age`\n",
    "---\n",
    "\n",
    "Let's simply impute all missing ages to be **999**. \n",
    "\n",
    "**NOTE**: This is not a best practice. However, \n",
    "1. Since we haven't really covered imputation in depth\n",
    "2. And the proper way would take too long to implement (thus detracting) from today's lecture\n",
    "3. And since we're ensembling with Decision Trees\n",
    "\n",
    "We'll do it this way as a matter of convenience."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age'] = train.Age.fillna(999)"
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Age']=train.Age.fillna(999)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Age'] = test.Age.fillna(999)"
=======
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Age']=test.Age.fillna(999)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: `Cabin`\n",
    "---\n",
    "\n",
    "Since there are so many missing values for `Cabin`, let's binarize that column as follows:\n",
    "- 1 if there originally was a value for `Cabin`\n",
    "- 0 if it was null\n",
    "\n",
    "**Do this for both `train` and `test`**"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 15,
=======
   "execution_count": 20,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 15,
=======
     "execution_count": 20,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Cabin.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Cabin = train.Cabin.notnull().astype(int)"
=======
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Cabin=train.Cabin.notnull().astype(int)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Cabin = test.Cabin.notnull().astype(int)"
=======
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Cabin=test.Cabin.notnull().astype(int)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Dummies\n",
    "---\n",
    "\n",
    "Dummy the `Sex` and `Embarked` columns. Be sure to set `drop_first=True`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['Sex', 'Embarked'], drop_first=True)"
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.get_dummies(train,columns=['Sex','Embarked'],drop_first=True)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.get_dummies(test, columns=['Sex', 'Embarked'], drop_first=True)"
=======
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.get_dummies(test,columns=['Sex','Embarked'],drop_first=True)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep: Create `X` and `y` variables\n",
    "---\n",
    "\n",
    "Our features will be:\n",
    "- `Pclass`\n",
    "- `Age`\n",
    "- `SibSp`\n",
    "- `Parch`\n",
    "- `Fare`\n",
    "- `Cabin`\n",
    "- `Sex_male`\n",
    "- `Embarked_Q`\n",
    "- `Embarked_S`\n",
    "\n",
    "And our target will be `Survived`"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']\n",
    "X = train[features]\n",
    "y = train['Survived']"
=======
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['Pclass','Age','SibSp','Parch','Fare','Cabin','Sex_male','Embarked_Q','Embarked_S']\n",
    "X=train[features]\n",
    "y=train['Survived']"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callenge: What is our baseline accuracy?\n",
    "---\n",
    "\n",
    "The baseline accuracy is the percentage of the majority class, regardless of whether it is 1 or 0. It serves as the benchmark for our model to beat."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
=======
   "execution_count": 26,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38245219347581555"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 22,
=======
     "execution_count": 26,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 24,
=======
   "execution_count": 27,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.617548\n",
       "1    0.382452\n",
       "Name: Survived, dtype: float64"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 24,
=======
     "execution_count": 27,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "---\n",
    "\n",
    "I know it can be confusing having an `X_test` from our training data vs a test set from Kaggle. If you want, you can use `X_val`/`y_val` for what we normally call `X_test`/`y_test`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42, stratify=y)"
=======
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(X,y, random_state=42,stratify=y)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model instantiation\n",
    "---\n",
    "\n",
    "Create an instance of `RandomForestClassifier` and `ExtraTreesClassifier`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)"
=======
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,random_state=42)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
=======
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier(random_state=42)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesClassifier(n_estimators=100)"
=======
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "et=ExtraTreesClassifier(n_estimators=100,random_state=42)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "---\n",
    "\n",
    "Which one has a higher `cross_val_score`?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 33,
=======
   "execution_count": 35,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "0.7732732732732733"
      ]
     },
     "execution_count": 33,
=======
       "0.8033105150937043"
      ]
     },
     "execution_count": 35,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "cross_val_score(dt, X_train, y_train, cv=3).mean()"
=======
    "cross_val_score(rf,X_train,y_train,cv=5).mean()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
=======
   "execution_count": 36,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "0.7958253843564134"
      ]
     },
     "execution_count": 28,
=======
       "0.7778363819997756"
      ]
     },
     "execution_count": 36,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "cross_val_score(rf, X_train, y_train, cv=3).mean()"
=======
    "cross_val_score(et,X_train,y_train,cv=5).mean()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 30,
=======
   "execution_count": 39,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "0.7867867867867867"
      ]
     },
     "execution_count": 30,
=======
       "0.7567613062507015"
      ]
     },
     "execution_count": 39,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "cross_val_score(et, X_train, y_train, cv=3).mean()"
=======
    "cross_val_score(dt,X_train,y_train,cv=5).mean()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "---\n",
    "\n",
    "They're both pretty close performance-wise. We could Grid Search over both, but for the sake of time we'll go with `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 34,
=======
   "execution_count": 40,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "0.8183183183183184\n"
=======
      "0.8198198198198198\n"
>>>>>>> Stashed changes
     ]
    },
    {
     "data": {
      "text/plain": [
<<<<<<< Updated upstream
       "{'max_depth': 4, 'n_estimators': 100}"
      ]
     },
     "execution_count": 34,
=======
       "{'max_depth': 4, 'n_estimators': 200}"
      ]
     },
     "execution_count": 40,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "}\n",
<<<<<<< Updated upstream
    "gs = GridSearchCV(rf, param_grid=rf_params, cv=5, n_jobs=-1)\n",
=======
    "gs = GridSearchCV(rf, param_grid=rf_params, cv=5)\n",
>>>>>>> Stashed changes
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Submission\n",
    "---\n",
    "\n",
    "Now that we've evaluated our model, let's submit our predictions to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
